{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749d8919",
   "metadata": {},
   "source": [
    "# NLP con librería NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e4d3c",
   "metadata": {},
   "source": [
    "En la instalación de **NLTK** no se incluyen los recursos necesarios para trabajar en PLN (las reglas de puntuación o las stopwords).    \n",
    "Por tanto, la primera vez que se ejecuten las funciones de librería se solicitará que se descarguen estos recursos.    \n",
    "Esto es algo que se puede hacer simplemente indicando a la función ```download()``` de *NLTK* los recursos requeridos.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474fc28d17e9201",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Análisis de sentimientos en inglés con NLTK.   \n",
    "\n",
    "Para realizar análisis de sentimientos con NLTK es necesario importar lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:45:32.776938Z",
     "start_time": "2024-03-11T13:45:32.771958Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Importamos librerías\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd70ba",
   "metadata": {},
   "source": [
    "### 1. Preprocesamiento de datos\n",
    "\n",
    "Antes de poder realizar el análisis de sentimientos con NLTK, es necesario preprocesar los mensajes de texto para normalizar. \n",
    "\n",
    "Los pasos a llevar a cabo son:\n",
    "\n",
    "- **Tokenización**: dividir el texto en palabras o frases más pequeñas llamadas tokens.\n",
    "- **Eliminación de signos de puntuación y caracteres especiales**.\n",
    "- **Conversión de texto a minúsculas** para normalizar el texto.\n",
    "- **Eliminación de las stopwords** o palabras irrelevantes para el mensaje tales como “a”, “el”, “y”, etc.\n",
    "- **Reducción de las palabras a su forma base (lemas)**.   \n",
    "\n",
    "Estos pasos se pueden implementar con funciones de NLTK, tal como se muestra en el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0cb0f7fb5e2df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:45:42.101426Z",
     "start_time": "2024-03-11T13:45:40.835424Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love content artificial intelligence blog article fantastic'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Procesado de texto\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "text = \"I love the content on the Artificial Intelligence blog, articles are fantastic.\"\n",
    "\n",
    "# Tokenización\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Eliminación de signos de puntuación\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "# Conversión a minúsculas\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Eliminación de stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Lematización\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Reconstrucción del texto preprocesado\n",
    "preprocessed_text = ' '.join(tokens)\n",
    "preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607f2b3",
   "metadata": {},
   "source": [
    "Proceso ejecutado:    \n",
    "\n",
    "- En primer lugar, se debe tokenizar las frase mediante la función ```word_tokenize()```, lo que divide está en una lista de palabras y signos de puntuación.    \n",
    "- Posteriormente, mediante con herramientas estándar de Python, se eliminan los tokens que estén en la lista de signos de puntuación (```string.punctuation```) y se convierten los todo el texto a minúsculas.    \n",
    "- Una vez homogeneizado el texto, se eliminan las stopwords que incluye NLTK.    \n",
    "- A la hora de importar las stopwords es necesario indicar el idioma con el que se está trabajando ya que estas son diferentes. \n",
    "- Finalmente se lematiza los tokens para eliminar plurales y derivaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fdd3a9",
   "metadata": {},
   "source": [
    "### 2. Extracción de características   \n",
    "\n",
    "Una vez preprocesado el texto, es necesario extraer las características de este antes de poder entrenar un modelo.    \n",
    "Lo más habitual es emplear la frecuencia de las palabras como características. En NLTK esto se implementa mediante la clase **FreqDist** y se muestra en el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e721298729c09180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:45:43.870604Z",
     "start_time": "2024-03-11T13:45:43.863419Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 1,\n",
       " 'content': 1,\n",
       " 'artificial': 1,\n",
       " 'intelligence': 1,\n",
       " 'blog': 1,\n",
       " 'article': 1,\n",
       " 'fantastic': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "features = {}\n",
    "words = word_tokenize(preprocessed_text)\n",
    "word_freq = FreqDist(words)\n",
    "\n",
    "for word, freq in word_freq.items():\n",
    "    features[word] = freq\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40f9c5",
   "metadata": {},
   "source": [
    "Obtenemos como resultado un diccionario con la palabra clave y el valor es el número de ocurrencias de cada una de estas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f2051",
   "metadata": {},
   "source": [
    "## 3. Conjunto de datos de entrenamiento   \n",
    "\n",
    "Para entrenar un modelo es necesario contar con un conjunto de datos de entrenamiento.   \n",
    "A tal efecto, se crea una lista de tuplas con el mensaje y la etiqueta que se desea que le corresponda para el entrenamiento.    \n",
    "\n",
    "A modo de ejemplo se puede probar con un listado de siete mensajes similar al siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20063f68b2a1481d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:50:11.954630Z",
     "start_time": "2024-03-11T13:50:11.948613Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"I love the content on the Artificial Intelligence blog, articles are fantastic.\", \"positive\"),\n",
    "    (\"The code does not work, it gave me an error when executing it.\", \"negative\"),\n",
    "    (\"I love this product!\", \"positive\"),\n",
    "    (\"This movie was terrible.\", \"negative\"),\n",
    "    (\"The weather is nice today.\", \"positive\"),\n",
    "    (\"I feel so sad about the news.\", \"negative\"),\n",
    "    (\"It's just an average book.\", \"neutral\"),\n",
    "    (\"I don´t like milk.\", \"negative\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e1205",
   "metadata": {},
   "source": [
    "### Factorización del código\n",
    "\n",
    "Se puede factorizar el código anterior para facilitar su uso a la hora de entrenar un modelo y que, globalmente, resulte más modular.    \n",
    "\n",
    "Para ello se pueden crear dos funciones:    \n",
    "- ```preprocess_text()``` para el preprocesado de texto y \n",
    "- ```extract_features()``` para la extracción de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d59c5cad495e7ff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:50:13.178009Z",
     "start_time": "2024-03-11T13:50:13.171944Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Realiza el preprocesamiento básico de un texto en inglés utilizando NLTK.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto a ser preprocesado.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto preprocesado.\n",
    "    \"\"\"\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Eliminación de signos de puntuación\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # Conversión a minúsculas\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Eliminación de stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lematización\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Reconstrucción del texto preprocesado\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    Extrae las características del texto utilizando NLTK y devuelve un diccionario de características.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto del cual extraer características.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario que representa las características extraídas del texto.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    words = word_tokenize(text)\n",
    "    word_freq = FreqDist(words)\n",
    "\n",
    "    for word, freq in word_freq.items():\n",
    "        features[word] = freq\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4069a3",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento del modelo.   \n",
    "\n",
    "El análisis de sentimientos con NLTK se puede realizar usando un clasificador basado en Naive Bayes.    \n",
    "NLTK proporciona una clase en la que se implementa este tipo de clasificadores.     \n",
    "\n",
    "Empleando esta clase y las funciones creadas en la sección anterior se puede entrenar un modelo con los datos de ejemplo, tal como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "378aafc2d7aed633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:50:16.361382Z",
     "start_time": "2024-03-11T13:50:16.350525Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# Preprocesamiento de los datos de entrenamiento\n",
    "preprocessed_training_data = [(preprocess_text(text), label) for text, label in training_data]\n",
    "\n",
    "# Extracción de características de los datos de entrenamiento\n",
    "training_features = [(extract_features(text), label) for text, label in preprocessed_training_data]\n",
    "\n",
    "# Entrenamiento del clasificador Naive Bayes\n",
    "classifier = NaiveBayesClassifier.train(training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6d38f",
   "metadata": {},
   "source": [
    "## 5. Clasificación de nuevos textos.\n",
    "\n",
    "Una vez que el modelo está entrando, este se puede usar para clasificar nuevos textos.    \n",
    "Simplemente es necesario preprocesar y extraer las características de la nueva cadena de texto para realizar la predicción con el clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a7ef191ae36faec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:51:43.057688Z",
     "start_time": "2024-03-11T13:51:43.050866Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# Nuevo texto para clasificar\n",
    "# new_text = \"I really enjoy the concert\" # positivo\n",
    "# new_text = \"The concert was terrible\"     # negativo\n",
    "new_text = \"I really love the concert\"    # negativo\n",
    "\n",
    "# Preprocesamiento del nuevo texto\n",
    "preprocessed_text = preprocess_text(new_text)\n",
    "\n",
    "# Extracción de características del nuevo texto\n",
    "features = extract_features(preprocessed_text)\n",
    "\n",
    "# Clasificación del nuevo texto\n",
    "sentiment = classifier.classify(features)\n",
    "print(\"Sentiment:\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0e98b859b0d14",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Análisis de sentimientos en español  \n",
    "\n",
    "En este noteboook se ha explicado cómo hacer análisis de sentimiento en inglés. Si se usa el ejemplo para trabajar con texto en español, u otros idiomas, el resultado no será satisfactorio dado que se ha usado el listado de stopwords del inglés y un lematizador (*WordNetLemmatizer*) que no es adecuado para el español.\n",
    "\n",
    "Por eso, para realizar análisis de sentimientos en español se dispone de otro notebook que usa la librería **spaCy**, la cúal dispone de las herramientas adecuadas para llevar a cabo correctamente esta tarea.   \n",
    "\n",
    "##  Conclusiones.\n",
    "NLTK es la librería de referencia para el procesado del lenguaje natural (PLN). Se trata de una librería que facilita el trabajo cuando se desea realizar análisis de sentimientos.    \n",
    "Aunque, como ya hemos dicho, funciones clave como la lematización solamente funcionan en inglés, el uso de NLTK facilita comprender los pasos necesarios para realizar este tipo de análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bfd4cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
