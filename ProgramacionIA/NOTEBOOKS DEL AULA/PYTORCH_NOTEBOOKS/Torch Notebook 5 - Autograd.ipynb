{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AUTOGRAD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ad83b791ababcfb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "En los últimos dos años hemos visto como PyTorch, el framework basado en Torch para Deep Learning creado por Facebook, está siendo una alternativa potente a Tensorflow.\n",
    "\n",
    "Una de las principales ventajas de PyTorch es que es un framework imperativo, en el que las operaciones/computaciones se van realizando conforme se ejecuta el programa. Tensorflow es en origen declarativo, el grafo se define primero y para cada ejecución se compila asignándole unos valores de entrada.\n",
    "\n",
    "La clave para tener entornos de deep learning eficientes es la diferenciación automática, un procedimiento de cálculo automático de las derivadas en un punto. En redes neuronales profundas, esta diferenciación es necesaria para el descenso por gradiente y como hemos visto en otros posts, se realiza usando propagación hacia atrás (backpropagation).\n",
    "\n",
    "PyTorch implementa la diferenciación automática con el paquete Autograd y el uso de variables. Las variables en PyTorch son nodos en el grafo computacional que almacenan:\n",
    "\n",
    "- Datos.\n",
    "- El gradiente de una función respecto a la variable.\n",
    "- La función que crea la variable.   \n",
    "\n",
    "¿Cómo computa Autograd los gradientes?\n",
    "\n",
    "Sigue un procedimiento en varias fases:\n",
    "\n",
    "- Se construye el grafo computacional haciendo un seguimiento de las operaciones realizadas en el flujo hacia delante de la red neuronal (forward pass). Mientras en Tensorflow, como hemos comentado, el grafo lo construye explícitamente el usuario, en PyTorch se construye de manera implícita.\n",
    "- Todas las operaciones se convierten en operaciones atómicas (sumas y multiplicaciones) para calcular las derivadas.\n",
    "- Se hace propagación hacia atrás con la regla de la cadena para calcular los gradientes respecto a los parámetros.   \n",
    "\n",
    "Vamos a ver un ejemplo de Autograd. A continuación se muestra el grafo a implementar con sus nodos y sus operaciones   \n",
    "\n",
    "\n",
    "<img src=\"./img/autograd.jpg\">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "338ffcc7061b7ec6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vemos por ejemplo como se calcularía el gradiente de la función de coste con respecto al primer parámetro w0 usando la regla de la cadena:\n",
    "<img src=\"./img/funcionCoste.png\">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4971beb70cf9cf6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se muestra el ejemplo en PyTorch del grafo mostrado, con las variables, las operaciones y la implementación de Autograd.\n",
    "\n",
    "Una vez declaradas las variables e implementado el grafo, llamando al método backward() sobre la función de coste, se calcula el gradiente respecto a las variables correspondientes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fbe5ca484616d83"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-03T18:43:47.438867Z",
     "start_time": "2024-11-03T18:43:45.698897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradiente de w0 respecto a L: -28.0\n",
      "Gradiente de w1 respecto a L: -21.0\n",
      "Gradiente de w2 respecto a L: -12.0\n",
      "Gradiente de w3 respecto a L: -23.0\n",
      "Gradiente de w4 respecto a L: -36.0\n"
     ]
    }
   ],
   "source": [
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    " \n",
    "# Leaf nodes are defined\n",
    "a1 = Variable(FloatTensor([4]))\n",
    "a2 = Variable(FloatTensor([3]))\n",
    " \n",
    "# Variables with gradients required are define\n",
    "w=Variable(FloatTensor([2,5,9,7,3]), requires_grad=True)\n",
    " \n",
    "b1 = w[0] * a1 + w[1] * a2 #b1=2*4+5*3=23\n",
    "b2 = w[2] * a1 #b2=9*4=36\n",
    "c = w[3] * b1 + w[4] * b2\n",
    "L = (15 - c)\n",
    " \n",
    "L.backward()\n",
    " \n",
    "gradient = w.grad.data\n",
    "for i in range(0, 5):\n",
    "    print(f\"Gradiente de w{i} respecto a L: {gradient[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como se puede comprobar, Autograd permite realizar la diferenciación automática y calcular los gradientes de manera sencilla."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3292bfcbd7b702e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "65336b5894ad16e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
