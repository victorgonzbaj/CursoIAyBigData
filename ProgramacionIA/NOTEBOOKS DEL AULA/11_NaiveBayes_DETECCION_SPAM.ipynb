{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## ALGORITMO NAIVE-BAYES MULTINOMIAL - DETECCIÓN DE SPAM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importamos librerías necesarias.  \n",
    "Cargamos dataset usando un URL y a través de un REQUESTS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T16:24:51.147125700Z",
     "start_time": "2024-02-13T16:24:49.385560200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   type                                            message\n0   ham                      Ok lar... Joking wif u oni...\n1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n2   ham  U dun say so early hor... U c already then say...\n3   ham  Nah I don't think he goes to usf, he lives aro...\n4  spam  FreeMsg Hey there darling it's been 3 week's n...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>spam</td>\n      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install requests\n",
    "import requests \n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "data_file = 'SMSSpamCollection'\n",
    "\n",
    "# Make request\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Get filename\n",
    "filename = url.split('/')[-1]\n",
    "\n",
    "# Download zipfile\n",
    "with open(filename, 'wb') as f:\n",
    "  f.write(resp.content)\n",
    "\n",
    "# Extract Zip\n",
    "with zipfile.ZipFile(filename, 'r') as zip:\n",
    "  zip.extractall('')\n",
    "\n",
    "# Read Dataset\n",
    "data = pd.read_table(data_file, \n",
    "                     header = 0,\n",
    "                     names = ['type', 'message']\n",
    "                     )\n",
    "\n",
    "# Show dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformación de datos\n",
    "Una vez tenemos los datos, lo primero de todo tenemos que procesarlos. Si bien el procesamiento de datos es un paso fundamental en todo proyecto de machine learning, en los proyectos de NLP (como este) procesar los datos resulta aún mucho más importante.\n",
    "\n",
    "Así pues, lo primero de todo vamos seguir los siguientes procesos:\n",
    "\n",
    "- **Tokenización**: consiste en separar los mensajes en palabras para así poder tratar cada una de las palabras. Esto lo podemos hacer gracias a la función word_tokenize.\n",
    "- **Eliminación de stop words**: eliminación de palabras que no aportan valor (preposiciones, conjunciones, etc.). Esto se realiza puesto que aumentaría mucho el tamaño de nuestro dataset (ya de por sí grande) y no aportaría ningún valor, solo ruido. Existen dos grandes fuentes de stpwords en Python, las de Sklearn y las de la librería NLTK."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jordi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'don',\n \"don't\",\n 'should',\n \"should've\",\n 'now',\n 'd',\n 'll',\n 'm',\n 'o',\n 're',\n 've',\n 'y',\n 'ain',\n 'aren',\n \"aren't\",\n 'couldn',\n \"couldn't\",\n 'didn',\n \"didn't\",\n 'doesn',\n \"doesn't\",\n 'hadn',\n \"hadn't\",\n 'hasn',\n \"hasn't\",\n 'haven',\n \"haven't\",\n 'isn',\n \"isn't\",\n 'ma',\n 'mightn',\n \"mightn't\",\n 'mustn',\n \"mustn't\",\n 'needn',\n \"needn't\",\n 'shan',\n \"shan't\",\n 'shouldn',\n \"shouldn't\",\n 'wasn',\n \"wasn't\",\n 'weren',\n \"weren't\",\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\"]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "#text.text.ENGLISH_STOP_WORDS\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T16:41:54.222739500Z",
     "start_time": "2024-02-13T16:41:52.107245Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Stemming** o **lemmatization**: el objetivo de este paso es que dos palabras que signifiquen lo mismo, pero no estén igual escritas, pasen a estar igual escritas. Al fin y al cabo, para el modelo la palabra «bueno» y la palabra «buena» son diferentes. Para hacer esto hay dos grandes técnicas:\n",
    "\n",
    "- Stemming: el stemming consiste en la eliminación de las terminaciones de las palabras para quedarnos únicamente con la raíz. Siguiendo el caso anterior, en ambos casos (bueno, buena) nos quedaríamos con «buen».\n",
    "- Lemmatization: es más utilizado en proyectos de NLP en inglés, ya que en este idioma bueno (good), mejor (better) y el mejor (best) son palabras completamente diferentes. En estos casos el stemming no funcionaría. Así pues, la lematización convertiría todas esas palabras a su base (good), de tal forma que pasen a significar lo mismo.  \n",
    "\n",
    "Para realizar todos estos procesos usaremos el paquete **nltk**, es decir, el Natural Language Toolkit, el cual incluye muchas funcionalidades sobre NLP para Python y que, sin duda, serán muy utiles para usar Naive Bayes en Python."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jordi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Tokenize\n",
    "data['tokens'] = data.apply(lambda x: nltk.word_tokenize(x['message']), axis = 1)\n",
    "\n",
    "# Remove stop words\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "# Apply Porter stemming\n",
    "stemmer = PorterStemmer()\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [stemmer.stem(item) for item in x])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T16:45:35.230070Z",
     "start_time": "2024-02-13T16:45:30.487458900Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una vez realizada la limpieza de nuestros datos, esto no acaba aquí. Y es que, actualmente únicamente disponemos de una columna que cuenta con un vector de palabras.\n",
    "\n",
    "Naive Bayes admite dos cuestiones:\n",
    "\n",
    "- Una matriz TF, es decir, una matriz en la que aparece, para cada documento, cuántas veces ha aparecido cada una de las palabras que hay en en todos los documentos.\n",
    "- Una matriz de apariciones. Es similar a una matriz TF, pero en este caso, en vez de indicar el número de apariciones, simplemente indica si esa palabra aparecía o no.  \n",
    "\n",
    "El uso de cada uno de ellas dependerá mucho del contexto. En el caso de SMS's, al ser mensajes muy cortos es poco probable que las palabras se repitan, por lo que seguramente ambos enfoques devuelvan el mismo resultado.\n",
    "\n",
    "Sin embargo, en textos más largos seguramente sea más interesante aplicar una matriz TF que una matriz de apariciones.\n",
    "\n",
    "Dicho esto, para poder llegar a una matriz TF vamos a hacer lo siguiente:\n",
    "\n",
    "- Destokenizar los valores, de tal forma que la columna «tokens» no contenga listas, sino texto. Esto es necesario para que el tercer paso funcione correctamente.\n",
    "- Hacer un split entre train y test. Es muy importante realizar el proceso de train y test antes de llegar a la matriz TF. Sino, tendremos problemas de data leakage y puede afectar a nuestro resultaod (incluso es nos permite comprobar que nuestro pipeline de datos es correcto).\n",
    "- Aplicar la función CountVectorizer del módulo feature_extraction.text de Sklearn a nuestros datos de train y test. Esta función nos permite crear la matriz TF o, si indicamos el parámetro binary = True, creará una matriz de apariciones.  \n",
    "\n",
    "Dicho esto, veamos cómo poder aplicar la matriz TF para poder entrenar el modelo de NLP con Naive Bayes en Python:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Unify the strings once again\n",
    "data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Make split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data['tokens'], \n",
    "    data['type'], \n",
    "    test_size= 0.2\n",
    "    )\n",
    "\n",
    "# Create vectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents = 'ascii', \n",
    "    lowercase = True\n",
    "    )\n",
    "\n",
    "# Fit vectorizer & transform it\n",
    "vectorizer_fit = vectorizer.fit(x_train)\n",
    "x_train_transformed = vectorizer_fit.transform(x_train)\n",
    "x_test_transformed = vectorizer_fit.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T16:46:30.855243100Z",
     "start_time": "2024-02-13T16:46:30.100105300Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Construcción y entrenamiento del modelo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Train Scores\n",
      "Balanced Accuracy: 0.9841222942124109\n",
      "Confussion Matrix:\n",
      " [[3857   11]\n",
      " [  17  571]]\n",
      "\n",
      "\n",
      "## Test Scores\n",
      "Balanced Accuracy: 0.9696060629983422\n",
      "Confussion Matrix:\n",
      " [[952   4]\n",
      " [  9 150]]\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train the model\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes_fit = naive_bayes.fit(x_train_transformed, y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "# Make predictions\n",
    "train_predict = naive_bayes_fit.predict(x_train_transformed)\n",
    "test_predict = naive_bayes_fit.predict(x_test_transformed)\n",
    "\n",
    "def get_scores(y_real, predict):\n",
    "  ba_train = balanced_accuracy_score(y_real, predict)\n",
    "  cm_train = confusion_matrix(y_real, predict)\n",
    "\n",
    "  return ba_train, cm_train \n",
    "\n",
    "def print_scores(scores):\n",
    "  return f\"Balanced Accuracy: {scores[0]}\\nConfussion Matrix:\\n {scores[1]}\"\n",
    "\n",
    "train_scores = get_scores(y_train, train_predict)\n",
    "test_scores = get_scores(y_test, test_predict)\n",
    "\n",
    "\n",
    "print(\"## Train Scores\")\n",
    "print(print_scores(train_scores))\n",
    "print(\"\\n\\n## Test Scores\")\n",
    "print(print_scores(test_scores))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T16:46:54.670084900Z",
     "start_time": "2024-02-13T16:46:54.556393500Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
