{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM en Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una LSTM (Long short-term memory) es un tipo de red neuronal recurrente (RNN) muy usada en problemas con dependencia temporal, que se compone de una unidad de memoria y tres reguladores que controlan el flujo de información a la unidad de memoria, *“input gate”*, *“output gate”* y *“forget gate”*.\n",
    "\n",
    "En PyTorch, las LSTM se implementan con la clase $torch.nn.LSTM(*args, **kwargs)$. La clase tiene los siguientes parámetros, entre otros:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dimensión de la entrada.\n",
    "- Dimensión del estado oculto h.\n",
    "- Número de capas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La entrada tiene la siguiente forma $(input, (h_0, c_0))$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La entrada $input$ es el tensor de la forma (longitud de la secuencia, número de instancias batch, dimensión de la entrada).\n",
    "- $h_0$ es el estado oculto inicial.\n",
    "- $c_0$ es el estado inicial de la unidad de memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida tiene la siguiente forma $(output, (h_n, c_n))$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La salida $output$ es el tensor de la forma (longitud de la secuencia, número de instancias batch, num_directions * dimensión del estado oculto) con la salida para cada instante de la secuencia.\n",
    "- Tensor $h_n$ con la forma (num_capas * num_directions, batch, dimensión estado oculto) que contiene el estado oculto en el último instante.\n",
    "- Tensor $c_n$ con la forma (num_capas * num_directions, batch, dimensión estado oculto) que contiene el estado de la unidad de memoria en el último instante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se puede ver una representación gráfica del tensor entrada de rango 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\img\\PyTorch2Tensor.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Creamos una instancia con una entrada de 10 dimensiones, una capa oculta de 6 dimeniones y 2 capas\n",
    "LSTM1 = nn.LSTM(10, 6, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la entrada, un batch de dos instancias, cada una con una secuencia de 5 time steps y dimensión 10\n",
    "input = torch.randn(5, 2, 10)\n",
    "\n",
    "#Inicializamos los estados iniciales para cada una de las instancias y cada una de las dos capas\n",
    "h0 = torch.randn(2, 2, 6)\n",
    "c0 = torch.randn(2, 2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5934,  0.0155,  0.1196,  0.0407,  0.0676,  0.1931],\n",
      "         [-0.1496, -0.5543,  0.0797, -0.1591, -0.0733,  0.3275]],\n",
      "\n",
      "        [[-0.3263,  0.0334,  0.0785,  0.0651,  0.0395,  0.1345],\n",
      "         [-0.2362, -0.2622,  0.0072, -0.1432, -0.0045,  0.2451]],\n",
      "\n",
      "        [[-0.3062,  0.0715, -0.0997,  0.1094,  0.0582,  0.1172],\n",
      "         [-0.2725, -0.0845, -0.1042, -0.0459,  0.0062,  0.2140]],\n",
      "\n",
      "        [[-0.2948,  0.0464, -0.1396,  0.0958,  0.0729,  0.1254],\n",
      "         [-0.2818,  0.0386, -0.1350,  0.0344,  0.0229,  0.1849]],\n",
      "\n",
      "        [[-0.3089,  0.0873, -0.1496,  0.1186,  0.0499,  0.1251],\n",
      "         [-0.2585,  0.1163, -0.1173,  0.0862,  0.0136,  0.1692]]],\n",
      "       grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "torch.Size([5, 2, 6])\n",
      "tensor([[[-0.0083,  0.2209,  0.1965, -0.1308,  0.1108,  0.1747],\n",
      "         [-0.0994,  0.3030, -0.3539, -0.0622, -0.3054,  0.1462]],\n",
      "\n",
      "        [[-0.3089,  0.0873, -0.1496,  0.1186,  0.0499,  0.1251],\n",
      "         [-0.2585,  0.1163, -0.1173,  0.0862,  0.0136,  0.1692]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "torch.Size([2, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "#Llamamos al método forward de la instancia creada, pasándole la entrada y los estados iniciales creados\n",
    "output, (hn, cn) = LSTM1(input, (h0, c0))\n",
    "\n",
    "#La salida output tiene la forma (seq_len, batch, num_directions * hidden_size) y contiene la salida h_t de la última capa para todos los intantes de tiempo para cada instancia del batc.\n",
    "print(output)\n",
    "print(output.shape)\n",
    "\n",
    "#La salida hn tiene la forma (num_layers * num_directions, batch, hidden_size) y contiene h_t para el último intantes de tiempo en cada capa.\n",
    "#Como se puede ver el último time step de la salida output coincide con la última capa de la salida hn\n",
    "print(hn)\n",
    "print(hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
