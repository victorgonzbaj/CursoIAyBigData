{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Redes Neuronales con Pytorch I. (torch.nn.Module)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "653830787971889"
  },
  {
   "cell_type": "markdown",
   "source": [
    "El paquete **torch.nn** de PyTorch contiene multitud de clases que nos permiten crear de una manera intuitiva redes neuronales y a la vez tener un nivel de detalle y control de los componentes de las mismas.\n",
    "\n",
    "Una de las clases más importantes de **torch.nn** es ***torch.nn.Module*** ya que es muy útil que los modelos que definamos hereden esta clase.   \n",
    "Para ello es necesario definir el constructor del modelo en *init()* instanciando los componentes que deseemos y sobreescribir el método *forward*, que realiza las operaciones forward del modelo y calcula la salida.  \n",
    "Con *super.init()* inicializamos la superclase nn.Module. El método forward se ejecuta directamente al llamar al modelo gracias al método call.\n",
    "\n",
    "En el siguiente ejemplo creamos un modelo basándonos en la clase nn.Module, instanciamos los componentes del modelo usando las clases del paquete torch.nn nn.Linear para la capa lineal y nn.Relu para la función de activacíon ReLU y definimos el flujo de las operaciones en el método forward. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "668bee567b36b999"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-03T18:07:56.065198Z",
     "start_time": "2024-11-03T18:07:54.432849Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  \n",
    "        self.relu = nn.ReLU()                          \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) \n",
    "    \n",
    "    def forward(self, x):                              \n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0804, -0.1233,  0.1534, -0.3510, -0.1157,  0.1984, -0.0913,  0.0540,\n",
      "        -0.0598,  0.0405], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Creamos una instancia del modelo\n",
    "modelo1=Net(100,200,10)\n",
    "#Calculamos la salida con una entrada ejemplo\n",
    "x=torch.rand(100)\n",
    "output=modelo1(x)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T18:08:08.543909Z",
     "start_time": "2024-11-03T18:08:08.536079Z"
    }
   },
   "id": "a2108653c6d32d28",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dado que el modelo extiende la clase base nn.Module, es posible usar la gran cantidad de métodos y atributos de esta clase"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca3f6191509afe03"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Vemos la estructura del modelo\n",
    "print(modelo1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T18:09:02.672776Z",
     "start_time": "2024-11-03T18:09:02.669108Z"
    }
   },
   "id": "2fee57109b35d91e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#Movemos todos los parámetros del modelo a la GPU (Solo equipos dotados y configurados con GPU)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodelo1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PytorchBasics\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1050\u001B[0m, in \u001B[0;36mModule.cuda\u001B[1;34m(self, device)\u001B[0m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[0;32m   1035\u001B[0m \n\u001B[0;32m   1036\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1050\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PytorchBasics\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    898\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    899\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 900\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    902\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    903\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    904\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    905\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    910\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    911\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PytorchBasics\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    923\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    924\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    925\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    926\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 927\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    928\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    930\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PytorchBasics\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1050\u001B[0m, in \u001B[0;36mModule.cuda.<locals>.<lambda>\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[0;32m   1035\u001B[0m \n\u001B[0;32m   1036\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1050\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\PytorchBasics\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:310\u001B[0m, in \u001B[0;36m_lazy_init\u001B[1;34m()\u001B[0m\n\u001B[0;32m    305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    308\u001B[0m     )\n\u001B[0;32m    309\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 310\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    313\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    314\u001B[0m     )\n",
      "\u001B[1;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "#Movemos todos los parámetros del modelo a la GPU (Solo equipos dotados y configurados con GPU)\n",
    "modelo1.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T18:09:48.879153Z",
     "start_time": "2024-11-03T18:09:48.547526Z"
    }
   },
   "id": "6d5d6ae01cd91f1e",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc2.weight tensor([[ 0.0534,  0.0502,  0.0117,  ..., -0.0082,  0.0370,  0.0386],\n",
      "        [-0.0621,  0.0400,  0.0507,  ..., -0.0266,  0.0499, -0.0456],\n",
      "        [-0.0034,  0.0228,  0.0232,  ..., -0.0348, -0.0193,  0.0220],\n",
      "        ...,\n",
      "        [-0.0411, -0.0395,  0.0591,  ...,  0.0124,  0.0630,  0.0258],\n",
      "        [-0.0682,  0.0407, -0.0703,  ...,  0.0021,  0.0348,  0.0061],\n",
      "        [ 0.0478,  0.0483, -0.0497,  ...,  0.0677,  0.0651, -0.0331]])\n"
     ]
    }
   ],
   "source": [
    "#El método named_parameters() devuelve un iterador a los parámetros del modelo, mostrando el nombre del parámetro y el parámetro.\n",
    "for name, param in modelo1.named_parameters():\n",
    "  if param.requires_grad:\n",
    "    if name in ['fc2.weight']:\n",
    "        print (name, param.data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T18:10:06.459647Z",
     "start_time": "2024-11-03T18:10:06.435739Z"
    }
   },
   "id": "9642a0519eeefcb1",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc2.weight tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#El método apply(fn) aplica la función fn recursivamente en cada submódulo del modelo.\n",
    "#Vamos a crear una función que inicializa los parámetros de las capas lineales y aplicarlo al modelo.\n",
    "\n",
    "def init_w(m):\n",
    "  if type(m) == nn.Linear:\n",
    "    m.weight.data.fill_(1.0)\n",
    "\n",
    "modelo1.apply(init_w)\n",
    "\n",
    "#Vemos los parámetros de la segunda capa lineal después de aplicarles la función de inicialización.\n",
    "for name, param in modelo1.named_parameters():\n",
    "  if param.requires_grad:\n",
    "    if name in ['fc2.weight']:\n",
    "        print (name, param.data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T18:10:23.953644Z",
     "start_time": "2024-11-03T18:10:23.937184Z"
    }
   },
   "id": "c0a6925430de4631",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2296, -0.1637, -0.1033, -0.1710,  0.0052,  0.1170],\n",
      "         [-0.3022, -0.1191,  0.0098, -0.1651, -0.0720,  0.1079]],\n",
      "\n",
      "        [[-0.1971, -0.1419, -0.0175, -0.2366, -0.0481,  0.1676],\n",
      "         [-0.2725,  0.0135,  0.0574, -0.0566, -0.1141,  0.2074]],\n",
      "\n",
      "        [[-0.1964, -0.0802,  0.0521, -0.1611, -0.0998,  0.2026],\n",
      "         [-0.2584,  0.0119,  0.0519, -0.0776, -0.1616,  0.2448]],\n",
      "\n",
      "        [[-0.2135, -0.0512,  0.0546, -0.1454, -0.0877,  0.2124],\n",
      "         [-0.2388, -0.0027,  0.0570, -0.0942, -0.1688,  0.2494]],\n",
      "\n",
      "        [[-0.1930, -0.0182,  0.0784, -0.1430, -0.1369,  0.2326],\n",
      "         [-0.2252, -0.0058,  0.0731, -0.0909, -0.1731,  0.2604]]],\n",
      "       grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "tensor([[[-0.0729,  0.4497, -0.0678, -0.0376, -0.0460, -0.0429],\n",
      "         [ 0.0661,  0.1023, -0.1907,  0.0941, -0.2464, -0.1178]],\n",
      "\n",
      "        [[-0.1930, -0.0182,  0.0784, -0.1430, -0.1369,  0.2326],\n",
      "         [-0.2252, -0.0058,  0.0731, -0.0909, -0.1731,  0.2604]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#El módulo torch.nn también nos ofrece gran variedad de clases que definen las principales arquitecturas de deep learning.\n",
    "#Un ejemplo es torch.nn.LSTM() que se inicializa con los parámetros (input_size, hidden_size, num_layers).\n",
    "# Tiene como entrada (input,h_0,c_0) donde la forma de la entrada es (seq_len, batch, input_size).\n",
    "# La forma de los estados iniciales h_0 y c_0 es (num_layers * num_directions, batch, hidden_size) \n",
    "\n",
    "LSTM1 = nn.LSTM(10, 6, 2)\n",
    "input = torch.randn(5, 2, 10)\n",
    "h0 = torch.randn(2, 2, 6)\n",
    "c0 = torch.randn(2, 2, 6)\n",
    "output, (hn, cn) = LSTM1(input, (h0, c0))\n",
    "\n",
    "#La salida output tiene la forma (seq_len, batch, num_directions * hidden_size) y contiene la salida h_t para todos los intantes de tiempo.\n",
    "print(output)\n",
    "\n",
    "#La salida hn tiene la forma (num_layers * num_directions, batch, hidden_size) y contiene h_t para el último intantes de tiempo en cada capa.\n",
    "print(hn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T18:10:44.225476Z",
     "start_time": "2024-11-03T18:10:44.174420Z"
    }
   },
   "id": "fafab19b51802f98",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "853ea21b147afcba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
