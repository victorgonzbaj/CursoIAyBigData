{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## ANÁLISIS DE SENTIMIENTOS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3474fc28d17e9201"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-11T13:45:32.776938Z",
     "start_time": "2024-03-11T13:45:32.771958Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jordi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jordi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jordi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Jordi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importamos librerías\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'love content analytics lane blog article fantastic'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Procesado de texto\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "text = \"I love the content on the Analytics Lane blog, articles are fantastic.\"\n",
    "\n",
    "# Tokenización\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Eliminación de signos de puntuación\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "# Conversión a minúsculas\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Eliminación de stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Lematización\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Reconstrucción del texto preprocesado\n",
    "preprocessed_text = ' '.join(tokens)\n",
    "preprocessed_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T13:45:42.101426Z",
     "start_time": "2024-03-11T13:45:40.835424Z"
    }
   },
   "id": "9e0cb0f7fb5e2df0",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'love': 1,\n 'content': 1,\n 'analytics': 1,\n 'lane': 1,\n 'blog': 1,\n 'article': 1,\n 'fantastic': 1}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "features = {}\n",
    "words = word_tokenize(preprocessed_text)\n",
    "word_freq = FreqDist(words)\n",
    "\n",
    "for word, freq in word_freq.items():\n",
    "    features[word] = freq\n",
    "\n",
    "features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T13:45:43.870604Z",
     "start_time": "2024-03-11T13:45:43.863419Z"
    }
   },
   "id": "e721298729c09180",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"I love the content on the Analytics Lane blog, articles are fantastic.\", \"positive\"),\n",
    "    (\"The code does not work, it gave me an error when executing it.\", \"negative\"),\n",
    "    (\"I love this product!\", \"positive\"),\n",
    "    (\"This movie was terrible.\", \"negative\"),\n",
    "    (\"The weather is nice today.\", \"positive\"),\n",
    "    (\"I feel so sad about the news.\", \"negative\"),\n",
    "    (\"It's just an average book.\", \"neutral\"),\n",
    "    (\"I don´t like milk.\", \"negative\")\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T13:50:11.954630Z",
     "start_time": "2024-03-11T13:50:11.948613Z"
    }
   },
   "id": "20063f68b2a1481d",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Realiza el preprocesamiento básico de un texto en inglés utilizando NLTK.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto a ser preprocesado.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto preprocesado.\n",
    "    \"\"\"\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Eliminación de signos de puntuación\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # Conversión a minúsculas\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Eliminación de stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lematización\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Reconstrucción del texto preprocesado\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    Extrae las características del texto utilizando NLTK y devuelve un diccionario de características.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto del cual extraer características.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario que representa las características extraídas del texto.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    words = word_tokenize(text)\n",
    "    word_freq = FreqDist(words)\n",
    "\n",
    "    for word, freq in word_freq.items():\n",
    "        features[word] = freq\n",
    "\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T13:50:13.178009Z",
     "start_time": "2024-03-11T13:50:13.171944Z"
    }
   },
   "id": "d59c5cad495e7ff4",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# Preprocesamiento de los datos de entrenamiento\n",
    "preprocessed_training_data = [(preprocess_text(text), label) for text, label in training_data]\n",
    "\n",
    "# Extracción de características de los datos de entrenamiento\n",
    "training_features = [(extract_features(text), label) for text, label in preprocessed_training_data]\n",
    "\n",
    "# Entrenamiento del clasificador Naive Bayes\n",
    "classifier = NaiveBayesClassifier.train(training_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T13:50:16.361382Z",
     "start_time": "2024-03-11T13:50:16.350525Z"
    }
   },
   "id": "378aafc2d7aed633",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# Nuevo texto para clasificar\n",
    "# new_text = \"I really enjoy the concert\" # positivo\n",
    "# new_text = \"The concert was terrible\"     # negativo\n",
    "new_text = \"I really love the concert\"    # negativo\n",
    "\n",
    "# Preprocesamiento del nuevo texto\n",
    "preprocessed_text = preprocess_text(new_text)\n",
    "\n",
    "# Extracción de características del nuevo texto\n",
    "features = extract_features(preprocessed_text)\n",
    "\n",
    "# Clasificación del nuevo texto\n",
    "sentiment = classifier.classify(features)\n",
    "print(\"Sentiment:\", sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T13:51:43.057688Z",
     "start_time": "2024-03-11T13:51:43.050866Z"
    }
   },
   "id": "2a7ef191ae36faec",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d1d0e98b859b0d14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
