{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCCIÓN AL NLP (NATURAL LANGUAGE PROCESSING - PROCESAMIENTO DE LENGUAJE NATURAL)\n",
    "\n",
    "## Conceptos básicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZACIÓN\n",
    "\n",
    "En un proceso de NLP una de las primeras tareas a realizar va a ser eliminar las palabras que tienen poco interés para nuestro análisis.   \n",
    "El primer paso es delimitar las palabras del texto, y convertir esas palabras en elementos de una lista. Este procedimiento es conocido como **tokenización**. \n",
    "\n",
    "Procedemos a implementar esto en Python: (Usaremos la librería ***SPACY***-> **pip install spacy**)\n",
    "\n",
    "Nota:    \n",
    "Ejecutar en terminal: \n",
    "     \n",
    "***python -m spacy download es***   \n",
    "\n",
    "para descargar el modelo de idioma español-castellano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "text = \"\"\"Soy un texto. Normalmente soy más largo y más grande. Que no te engañe mi tamaño.\"\"\"\n",
    "doc = nlp(text) # Crea un objeto de spacy tipo nlp\n",
    "tokens = [t.orth_ for t in doc] # Crea una lista con las palabras del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, ya tenemos nuestro texto convertido en una lista de **tokens**.    \n",
    "\n",
    "Lamentablemente, están todas las palabras. Se puede comprobar que también se incluyen como **tokens** los signos de puntuación.    \n",
    "\n",
    "Por nuestra parte, lo que nos interesa es quedarnos solo con aquellas que sean más o menos representativas del texto. Para ello, vamos a eliminar de esa lista las palabras muy comunes o poco informativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a pedirle a esta librería que construya la lista de tokens pero que no incluya palabras muy comunes y poco informativas desde el punto de vista léxico, tales como conjunciones (y, o, ni, que), preposiciones (a, en, para, por, entre otras) y verbos muy comunes (ser, ir, y otros más).    \n",
    "\n",
    "Para realizarlo, se va a utilizar una condición que diga “todos los tokens del texto, siempre y cuando no se incluyan la puntuación ni las palabras poco representativas (stopwords)”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "text = \"\"\"Soy un texto. Normalmente soy más largo y más grande. Que no te engañe mi tamaño.\"\"\"\n",
    "doc = nlp(text)\n",
    "lexical_tokens = [t.orth_ for t in doc if not t.is_punct | t.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya disponemos de una lista de palabra que nos pueden dar una idea sobre la temática tratada en el texto o bien, que nos permita una clasificación más certera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMALIZACIÓN\n",
    "\n",
    "El siguiente paso en este flujo de trabajo consiste en normalizar el texto.       \n",
    "\n",
    "El tokenizador reconoce formas como *caminar, Caminar y CAMINAR* como formas distintas. Además, el documento puede tener números y palabras compuestas por caracteres alfanuméricos y otros símbolos tales como #Ar1anaG. Si no nos interesan estas palabras, y queremos que en la lista aparezcan solamente las formas convencionales (por ejemplo, caminar, sólo en minúsculas) debemos normalizar nuestro texto.    \n",
    "\n",
    "Aprovecharemos el **momentum** para descartar palabras muy cortas (menores a 4 caracteres) para filtrar aún más nuestros tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [t.lower() for t in lexical_tokens if len(t) > 3 and t.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo agrupamos todo en una única función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['texto', 'prueba', 'tokens', 'quedarán', 'normalización']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "def normalize(text):\n",
    "    doc = nlp(text)\n",
    "    words = [t.orth_ for t in doc if not t.is_punct | t.is_stop]\n",
    "    lexical_tokens = [t.lower() for t in words if len(t) > 3 and     \n",
    "    t.isalpha()]\n",
    "    return lexical_tokens\n",
    "\n",
    "word_list = normalize(\"Soy un texto de prueba. ¿Cuántos tokens me quedarán después de la normalización?\")\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEMATIZACIÓN\n",
    "\n",
    "En español, por ejemplo, sabemos que canto, cantas, canta, cantamos, cantáis, cantan son distintas formas (conjugaciones) de un mismo verbo (cantar). Y que niña, niño, niñita, niños, niñotes, y otras más, son distintas formas del vocablo niño. Así que sería genial poder obviar las diferencias y juntar todas estas variantes en un mismo término.    \n",
    "Y eso es precisamente lo que hace la lematización: relaciona una palabra flexionada o derivada con su forma canónica o lema. Y un lema no es otra cosa que la forma que tienen las palabras cuando las buscas en el diccionario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ser', 'uno', 'texto', 'que', 'pedir', 'a', 'grito', 'que', 'él', 'procesen', '.', 'por', 'ese', 'yo', 'cantar', ',', 'tú', 'canta', ',', 'él', 'cantar', ',', 'yo', 'cantar', ',', 'cantáis', ',', 'cantar', '…']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "text = \"\"\"Soy un texto que pide a gritos que lo procesen. Por eso yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…\"\"\"\n",
    "doc = nlp(text)\n",
    "lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el proceso de lematización toma en consideración la probable clase de palabra (adjetivo, verbo, sustantivo…) — también llamados POS — es posible usar dicha información para filtrar la lista de lemas.   \n",
    "La siguiente línea excluye los pronombres de nuestra lista de lemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ser', 'uno', 'texto', 'pedir', 'a', 'grito', 'procesen', '.', 'por', 'cantar', ',', 'canta', ',', 'cantar', ',', 'cantar', ',', 'cantáis', ',', 'cantar', '…']\n"
     ]
    }
   ],
   "source": [
    "lemmas_no_pron = [tok.lemma_.lower() for tok in doc if tok.pos_ != 'PRON']\n",
    "print(lemmas_no_pron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lematización es un proceso clave en muchas tareas prácticas de PLN, pero tiene ***dos costos***:   \n",
    "- Primero, es un proceso que consume recursos (sobre todo tiempo). \n",
    "- Segundo, suele ser probabilística, así que en algunos casos se suelen obtener resultados inesperados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMING\n",
    "\n",
    "Es el procedimiento de convertir palabras en raíces. Estas raíces son la parte invariable de palabras relacionadas sobre todo por su forma.    \n",
    "\n",
    "En cierta manera se parece a la lematización, pero los resultados (las raíces) no tienen por qué ser palabras de un idioma.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'pid', 'grit', 'proces', 'cant', 'cant', 'cant', 'cant', 'cant', 'cant']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "spanishstemmer=SnowballStemmer('spanish')\n",
    "text = \"\"\"Soy un texto que pide a gritos que lo procesen. Por eso yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…\"\"\"\n",
    "tokens = normalize(text) # crear una lista de tokens\n",
    "stems = [spanishstemmer.stem(token) for token in tokens]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, para encontrar las raíces en español nos hemos valido de otra librería de Python llamada **nltk**. Es otra librería fundamental para el procesamiento de lenguaje natural.    \n",
    "\n",
    "En **nltk** hay muchas funciones para tareas de este tipo, en varios idiomas. En el ejemplo se ha utilizado el ***Snowball Stemmer*** porque funciona no sólo en inglés, sino en otras lenguas como el español."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El stemming es mucho más rápido desde el punto de vista del procesamiento que la lematización. También tiene como ventaja que reconoce relaciones entre palabras de distinta clase. Podría reconocer, por ejemplo, que picante y picar tienen como raíz pic-. En otras palabras, el stemming puede reducir el número de elementos que forman nuestros textos. Y eso, en muchos casos, es lo se suele tener como objetivo.\n",
    "\n",
    "Por otra parte, una desventaja del stemming es que sus algoritmos son más simples que los de lematización. Pueden “recortar” demasiado la raíz y encontrar relaciones entre palabras que realmente no existen (overstemming).    \n",
    "También puede suceder que deje raíces demasiado extensas o específicas, y que tengamos más bien un déficit de raíces (understemming), en cuyo caso palabras que deberían convertirse en una misma raíz no lo hacen. No hay mucho que hacer con eso, pero el stemming es una muy buena solución de compromiso en la mayoría de los casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión   \n",
    "\n",
    "La **tokenización**, **normalización**, **lematización** y **“radicalización”(stemming)** de un texto suelen ser procedimientos fundamentales para muchas tareas relacionadas con la extracción automática de características y de datos de los textos.    \n",
    "Estos procedimientos están en la base de los grandes y pequeños buscadores de información.    \n",
    "\n",
    "El **stemming** suele ser una buena solución cuando no importa demasiado la precisión y se requiere de un procesamiento eficiente.    \n",
    "La **lematización** suele funcionar mejor cuando se necesita procesar palabras de manera similar a como lo hace un ser humano."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
